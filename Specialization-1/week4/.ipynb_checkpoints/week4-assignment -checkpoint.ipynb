{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-layer network. \n",
    "\n",
    "* We will work on classifying cat pictures with L layers of neural network \n",
    "* First, we will write some function that will help make the complete model \n",
    "* Broadly, we will follow the steps below: \n",
    "    - Initialize parameters for all layers \n",
    "    - Forward propagation for Linear -> ReLu layers, i.e. from layer 1 to laye L-1\n",
    "    - Forward propagation for Linear -> Sigmoid layers, i.e. for layer L.\n",
    "    - Compute cost function(cross entropy loss)\n",
    "    - Backward propagation for Linear -> Sigmoid layer L.\n",
    "    - Backward propagation for Linear -> ReLu layers, i.e. from L-1 to 1.\n",
    "    - we update parameter at each step during this process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * We will be using following function for this work \n",
    "     - Initilaize_parameters \n",
    "     - L_model_forward \n",
    "     - Compute_cost \n",
    "     - L_model_backward \n",
    "     - Update_parameters \n",
    "     - sigmoid \n",
    "     - Relu \n",
    "     - Sigmoid_backward \n",
    "     - ReLu_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions for different layers \n",
    "\n",
    "## Relu and its back prop implementation \n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement ReLu activation fucntion \n",
    "    \n",
    "    Argument: \n",
    "    Z : array of matrix \n",
    "    \n",
    "    Return : \n",
    "    A : a rectified linear unit output. \n",
    "    Activation_cache : cached value of Z in a dictionary, that will be used during back propagation \n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z) \n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement relu backward for single unit \n",
    "    \n",
    "    Arguments: \n",
    "    dA : \n",
    "    cache : \n",
    "    \n",
    "    Returns: \n",
    "    dZ : drivative of the relu function \n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache \n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    # for relu, we only need to take care of the less than zero terms and set them to zero \n",
    "    dZ[Z<=0] = 0\n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "# sigmoid and its backprop implementation\n",
    "\n",
    "def sigmoid(Z): \n",
    "    \"\"\"\n",
    "    Implement sigmoid function on array. \n",
    "    \n",
    "    Argument: \n",
    "    Z : weighted input array before activation \n",
    "    \n",
    "    Returns: \n",
    "    A : activation value A after application of sigmoid. \n",
    "    cache : value of Z, cached for effective calculation during back propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache): \n",
    "    \"\"\"\n",
    "    Implement derivative of sigmoid. \n",
    "    \n",
    "    Arguments: \n",
    "    A : post active gradient of any shape \n",
    "    Cache : value Z cached during sigmoid calculation\n",
    "    \n",
    "    Returns : \n",
    "    Z : derivative of sigmoid, here it will return derivative of Cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache \n",
    "    sigmoid = 1/ (1 + np.exp(-Z))\n",
    "    dZ = dA*sigmoid*(1-sigmoid)\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# a function to load data, \n",
    "# credit: deeplearnin.ai module 1 week 4 assignments \n",
    "\n",
    "def load_data():\n",
    "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter initialization \n",
    "\n",
    "def initialize_parameters_deep(layer_dims): \n",
    "    \"\"\"\n",
    "    Initialize the parameters for each layer of L-layer network \n",
    "    \n",
    "    Argument: \n",
    "    layer_dims : it has all the dimension needed for inintializing the parameters \n",
    "    \n",
    "    Returns: \n",
    "    parameters: a dictionary of dw and db after initialization\n",
    "    \"\"\"\n",
    "    \n",
    "    # to make the parameters same on random initialization.\n",
    "    np.random.seed(1)\n",
    "    # the dictionary which will be returned.\n",
    "    parameters = {}\n",
    "    # number of layers in the network is denoted by L\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        # W.shape == (#hidden units, #output from last layer)\n",
    "        # two different ways of initilization\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "#         parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        \n",
    "        # b.shape == (#hidden units, 1)\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        # double checking \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear forward and linear activation forward functions for implementing forward propagation, \n",
    "## this function will be used insdie the L_model_forward for calculating value from 1 to L layers.\n",
    "\n",
    "def linear_forward(A, W, b): \n",
    "    \"\"\"\n",
    "    Implement linear part during forward propagation \n",
    "    \n",
    "    Arguments: \n",
    "    A : activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W : weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b : bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    \n",
    "    Returns : \n",
    "    Z : the input of the activation function, also called pre-activation parameter \n",
    "    cache : a python dictionary containing \"A\", \"W\" and \"b\" \n",
    "    \"\"\"\n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement activation work during forward propagation \n",
    "    \n",
    "    Arguments: \n",
    "    A_prev : activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W : weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b : bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation : \"sigmoid\" or \"relu\" activation depending upon the layer for which we are calculating.\n",
    "    \n",
    "    Returns : \n",
    "    A : The activation calculated for given arguments.\n",
    "    cache: a cache which is combination of linear_activation cache and Activation cache binded in a tuple as \n",
    "            (linear_cache, activation_cache)\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # for calculating the input of activation we call linear_forward function\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"Z, linear_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        # for calculating activation we call sigmoid function \n",
    "        # inputs: Z, Outputs: A, activation_cache\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # for calculating the input of activation we call linear_forward function\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"Z, linear_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        # for calculating activation we call ReLu function \n",
    "        # inputs: Z, Outputs: A, activation_cache\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    \n",
    "    # cache tuple : (linear_cache, activation_cache) -> ((A, W, b), Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## L-model forward function \n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for L-layer model \n",
    "    \n",
    "    Arguments : \n",
    "    X : import images, flattened. \n",
    "    parameters : randomly initialized parameters. \n",
    "    \n",
    "    Returns: \n",
    "    AL : final layer output \n",
    "    caches : caches from linear_acitvation_forward function, which has Z values as caches.\n",
    "            in the following formate: (linear_cache, activation_cache) -> ((A, W, b), Z)\n",
    "    \"\"\"\n",
    "    # this will store all caches which comes from linear_activation_forward function for each iteration.\n",
    "    # at each iteration, the valeus of caches will be added at the end of list. \n",
    "    # since we are iterating from 1 to L-1, for relu, while accessing the cache values we will need to index l-1.\n",
    "    caches = []\n",
    "    \n",
    "    # to keep the conformity with the notations we use, we assign X to A.\n",
    "    A = X\n",
    "    \n",
    "    # for each layers we have two set of parameters, w and b,  so dividing by 2 will give the number of layers.\n",
    "    L = len(parameters) // 2                 \n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). \n",
    "    # caches from each layer is added to caches list at l-1 index.\n",
    "    for l in range(1, L):\n",
    "        # As we iterate, we need to use last layer activation in next layer so we keep reassigning values of A to A_prev\n",
    "        A_prev = A \n",
    "        # to calculate linear activation for a layer, we call linear_activation_forward function \n",
    "        # which in turn calls linear_forward function to do the calculation. \n",
    "        # it returns cache values in form ((A, W, b), Z), we append it in \"caches\".\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        # append into caches, its index will be l-1 when we try to fetch for layer l.\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID.\n",
    "    # A similar process as above is repeated here. this time for output layer AL.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost calculation\n",
    "\n",
    "    Arguments:\n",
    "    AL : output of the final layer to calculate cost\n",
    "    Y : true \"label\"\n",
    "\n",
    "    Returns:\n",
    "    cost : total cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from AL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    cost = np.squeeze(cost)      \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation one layers at a time.\n",
    "\n",
    "    Arguments:\n",
    "    dZ : Gradient of the cost with respect to input Z\n",
    "    cache : tuple of values of the linear_cache of the form  (A_prev, W, b).\n",
    "\n",
    "    Returns:\n",
    "    dA_prev : Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW : Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db : Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    # linear cache which has 3 values for correspoinding layers. \n",
    "    # (A_prev, W, b)\n",
    "    # just to point, we dont need b cache.\n",
    "    A_prev, W, b = cache \n",
    "    \n",
    "    # total number of examples.\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # calculation of dW\n",
    "    dW = (1./m) * np.dot(dZ,A_prev.T)\n",
    "    # calculation of db\n",
    "    db = (1./m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    # calculation of dA_prev i.e. for l-1 layers.\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    # sanity check\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the activation layer. \n",
    "    NOTE: since we are moving backward, we first calculate for L layer activation, then L-1 layer activation. \n",
    "    Layer L activation derivative is calculated explicitly as dAL, wrt. cost.\n",
    "    \n",
    "    Arguments:\n",
    "    dA : derivative of after current layer activation, this will be used to calculate activation layer gradient. \n",
    "    cache : tuple of values (linear_cache, activation_cache).\n",
    "    activation : \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev : Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW : Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db : Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \n",
    "    Flow of operations: \n",
    "    1. using dA, activation derivative from l+1 layers, which is calculated before\n",
    "    2. find derivative of the function first, to get dZ\n",
    "    3. then use this dZ to calculate, the current layer, i.e. layer l, dW and db and also calculate dA_prev to feed \n",
    "    it again for next iteration. \n",
    "    we reutrn dA_prev to L_model_backward, where it will be reassigned and used for l-1 layer calculation\n",
    "    \"\"\"\n",
    "    # cache = ((A, W, b), Z) all for same layer. unpacked here as \n",
    "    # linear_cache = (A, W, b), activation_cache = Z\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        # it gives the derivative with relu function\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        # call linear_backward to find derivatives.\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        # derivative of sigmoind funciton \n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        # current layer derivatives\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement complete backward propagation with internal calls to different functions.\n",
    "    \n",
    "    Arguments:\n",
    "    AL : output of final layer, denoted by L.\n",
    "    Y : true \"label\" \n",
    "    caches : list of caches containing ((A,W,b),Z) values for each layer during forward propagation.\n",
    "    \n",
    "    Returns:\n",
    "    grads : A dictionary with the gradients dA, dW and db\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    # the number of layers, but with index l-1 for layer l\n",
    "    L = len(caches) \n",
    "    # total number of examples.\n",
    "    m = AL.shape[1]\n",
    "    # after this line, Y is the same shape as AL\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    # Initializing the backpropagation, this is done by taking drivative of cost wrt final activation output.\n",
    "    # this will be fed as input for doing back propagation one by one.\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients.\n",
    "    # we only need caches for final layer output, which is stored at index L-1\n",
    "    current_cache = caches[L-1]\n",
    "    # this is sigmoid layer calculation, at each layer l, dA is calculated for layer l-1\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    ## at this location, all gradients for final layer is completed.  now we move to ReLu layer gradient calculation.\n",
    "    \n",
    "    for l in reversed(range(L-1)): # going from L-1 to 0\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # since we are iterating from L-1 to 0, the index of caches and layer will be same. \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [12288, 20, 7, 5, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.771749\n",
      "Cost after iteration 100: 0.672053\n",
      "Cost after iteration 200: 0.648263\n",
      "Cost after iteration 300: 0.611507\n",
      "Cost after iteration 400: 0.567047\n",
      "Cost after iteration 500: 0.540138\n",
      "Cost after iteration 600: 0.527930\n",
      "Cost after iteration 700: 0.465477\n",
      "Cost after iteration 800: 0.369126\n",
      "Cost after iteration 900: 0.391747\n",
      "Cost after iteration 1000: 0.315187\n",
      "Cost after iteration 1100: 0.272700\n",
      "Cost after iteration 1200: 0.237419\n",
      "Cost after iteration 1300: 0.199601\n",
      "Cost after iteration 1400: 0.189263\n",
      "Cost after iteration 1500: 0.161189\n",
      "Cost after iteration 1600: 0.148214\n",
      "Cost after iteration 1700: 0.137775\n",
      "Cost after iteration 1800: 0.129740\n",
      "Cost after iteration 1900: 0.121225\n",
      "Cost after iteration 2000: 0.113821\n",
      "Cost after iteration 2100: 0.107839\n",
      "Cost after iteration 2200: 0.102855\n",
      "Cost after iteration 2300: 0.100897\n",
      "Cost after iteration 2400: 0.092878\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data \n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "# run the model\n",
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
