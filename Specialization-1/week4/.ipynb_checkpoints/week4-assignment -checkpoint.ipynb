{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-layer network. \n",
    "\n",
    "* We will work on classifying cat pictures with L layers of neural network \n",
    "* First, we will write some function that will help make the complete model \n",
    "* Broadly, we will follow the steps below: \n",
    "    - Initialize parameters for all layers \n",
    "    - Forward propagation for Linear -> ReLu layers, i.e. from layer 1 to laye L-1\n",
    "    - Forward propagation for Linear -> Sigmoid layers, i.e. for layer L.\n",
    "    - Compute cost function(cross entropy loss)\n",
    "    - Backward propagation for Linear -> Sigmoid layer L.\n",
    "    - Backward propagation for Linear -> ReLu layers, i.e. from L-1 to 1.\n",
    "    - we update parameter at each step during this process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * We will be using following function for this work \n",
    "     - Initilaize_parameters \n",
    "     - L_model_forward \n",
    "     - Compute_cost \n",
    "     - L_model_backward \n",
    "     - Update_parameters \n",
    "     - sigmoid \n",
    "     - Relu \n",
    "     - Sigmoid_backward \n",
    "     - ReLu_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions for different layers \n",
    "\n",
    "## Relu and its back prop implementation \n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement ReLu activation fucntion \n",
    "    \n",
    "    Argument: \n",
    "    Z : array of matrix \n",
    "    \n",
    "    Return : \n",
    "    A : a rectified linear unit output. \n",
    "    Activation_cache : cached value of Z in a dictionary, that will be used during back propagation \n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z) \n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement relu backward for single unit \n",
    "    \n",
    "    Arguments: \n",
    "    dA : \n",
    "    cache : \n",
    "    \n",
    "    Returns: \n",
    "    dZ : drivative of the relu function \n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache \n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    # for relu, we only need to take care of the less than zero terms and set them to zero \n",
    "    dZ[Z<=0] = 0\n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid(Z): \n",
    "    \"\"\"\n",
    "    Implement sigmoid function on array. \n",
    "    \n",
    "    Argument: \n",
    "    Z : weighted input array before activation \n",
    "    \n",
    "    Returns: \n",
    "    A : activation value A after application of sigmoid. \n",
    "    cache : value of Z, cached for effective calculation during back propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache): \n",
    "    \"\"\"\n",
    "    Implement derivative of sigmoid. \n",
    "    \n",
    "    Arguments: \n",
    "    A : post active gradient of any shape \n",
    "    Cache : value Z cached during sigmoid calculation\n",
    "    \n",
    "    Returns : \n",
    "    Z : derivative of sigmoid, here it will return derivative of Cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache \n",
    "    sigmoid = 1/ (1 + np.exp(-Z))\n",
    "    dZ = dA*sigmoid*(1-sigmoid)\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
