{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-layer network. \n",
    "\n",
    "* We will work on classifying cat pictures with L layers of neural network \n",
    "* First, we will write some function that will help make the complete model \n",
    "* Broadly, we will follow the steps below: \n",
    "    - Initialize parameters for all layers \n",
    "    - Forward propagation for Linear -> ReLu layers, i.e. from layer 1 to laye L-1\n",
    "    - Forward propagation for Linear -> Sigmoid layers, i.e. for layer L.\n",
    "    - Compute cost function(cross entropy loss)\n",
    "    - Backward propagation for Linear -> Sigmoid layer L.\n",
    "    - Backward propagation for Linear -> ReLu layers, i.e. from L-1 to 1.\n",
    "    - we update parameter at each step during this process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * We will be using following function for this work \n",
    "     - Initilaize_parameters \n",
    "     - L_model_forward \n",
    "     - Compute_cost \n",
    "     - L_model_backward \n",
    "     - Update_parameters \n",
    "     - sigmoid \n",
    "     - Relu \n",
    "     - Sigmoid_backward \n",
    "     - ReLu_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions for different layers \n",
    "\n",
    "## Relu and its back prop implementation \n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement ReLu activation fucntion \n",
    "    \n",
    "    Argument: \n",
    "    Z : array of matrix \n",
    "    \n",
    "    Return : \n",
    "    A : a rectified linear unit output. \n",
    "    Activation_cache : cached value of Z in a dictionary, that will be used during back propagation \n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z) \n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement relu backward for single unit \n",
    "    \n",
    "    Arguments: \n",
    "    dA : \n",
    "    cache : \n",
    "    \n",
    "    Returns: \n",
    "    dZ : drivative of the relu function \n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache \n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    # for relu, we only need to take care of the less than zero terms and set them to zero \n",
    "    dZ[Z<=0] = 0\n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "# sigmoid and its backprop implementation\n",
    "\n",
    "def sigmoid(Z): \n",
    "    \"\"\"\n",
    "    Implement sigmoid function on array. \n",
    "    \n",
    "    Argument: \n",
    "    Z : weighted input array before activation \n",
    "    \n",
    "    Returns: \n",
    "    A : activation value A after application of sigmoid. \n",
    "    cache : value of Z, cached for effective calculation during back propagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache): \n",
    "    \"\"\"\n",
    "    Implement derivative of sigmoid. \n",
    "    \n",
    "    Arguments: \n",
    "    A : post active gradient of any shape \n",
    "    Cache : value Z cached during sigmoid calculation\n",
    "    \n",
    "    Returns : \n",
    "    Z : derivative of sigmoid, here it will return derivative of Cost.\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache \n",
    "    sigmoid = 1/ (1 + np.exp(-Z))\n",
    "    dZ = dA*sigmoid*(1-sigmoid)\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# a function to load data, \n",
    "# credit: deeplearnin.ai module 1 week 4 assignments \n",
    "\n",
    "def load_data():\n",
    "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter initialization \n",
    "\n",
    "def initialize_parameters(layer_dims): \n",
    "    \"\"\"\n",
    "    Initialize the parameters for each layer of L-layer network \n",
    "    \n",
    "    Argument: \n",
    "    layer_dims : it has all the dimension needed for inintializing the parameters \n",
    "    \n",
    "    Returns: \n",
    "    parameters: a dictionary of dw and db after initialization\n",
    "    \"\"\"\n",
    "    \n",
    "    # to make the parameters same on random initialization.\n",
    "    np.random.seed(1)\n",
    "    # the dictionary which will be returned.\n",
    "    parameters = {}\n",
    "    # number of layers in the network is denoted by L\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        # W.shape == (#hidden units, #output from last layer)\n",
    "        # two different ways of initilization\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        \n",
    "        # b.shape == (#hidden units, 1)\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        # double checking \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear forward and linear activation forward functions for implementing forward propagation, \n",
    "## this function will be used insdie the L_model_forward for calculating value from 1 to L layers.\n",
    "\n",
    "def linear_forward(A, W, b): \n",
    "    \"\"\"\n",
    "    Implement linear part during forward propagation \n",
    "    \n",
    "    Arguments: \n",
    "    A : activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W : weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b : bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    \n",
    "    Returns : \n",
    "    Z : the input of the activation function, also called pre-activation parameter \n",
    "    cache : a python dictionary containing \"A\", \"W\" and \"b\" \n",
    "    \"\"\"\n",
    "    Z = W.dot(A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## L-model forward function \n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for L-layer model \n",
    "    \n",
    "    Arguments : \n",
    "    X : import images, flattened. \n",
    "    parameters : randomly initialized parameters. \n",
    "    \n",
    "    Returns: \n",
    "    AL : final layer output \n",
    "    caches : caches from linear_acitvation_layer, which has Z values as caches.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
