{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient checking \n",
    "\n",
    "* Gradient checking is an effective tool to make sure the proper working of backward propagation. \n",
    "* We are only checking wokring of backward propagation since we are confident that our forward propagation implementation works fine. \n",
    "* First, we will implement 1-d gradient checking so as to build an intuitions about it then we will implement N-dimentional gradient checking.\n",
    "* This implementation is a three layer implementation with output layer having sigmoid activation and other layers having ReLu activation. \n",
    "* Network parameters (input = 4, 5, 3, output = 1)\n",
    "* Loss: cross entropy loss\n",
    "\n",
    "## understanding gradient checking process: \n",
    "* When we increase and decrease the same parameter values by epsilon, then we only calculate the gradapprox for only that parameter.\n",
    "* This means that, for each paramerters, we need to impelment complete forward propagation with thetaplus and again with thetaminus and then calculate the gradapprox value for that particular parameter. \n",
    "* For calculating gradappox of one parameter, we implement forward propagation twice, so for complete evaluation, we need to implement 2*number_of_parameters time forward propagation. \n",
    "* This requirememtn makes it costly to calculate, so this should not be done at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np \n",
    "from testCases import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting functions \n",
    "* There are certain function that we need to calculate forward propagation and backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid\n",
    "def sigmoid(x): \n",
    "    \"\"\"\n",
    "    calculates sigmoid of x.\n",
    "    \n",
    "    Argument: \n",
    "    x : A scalar or numpy array of any size. \n",
    "    \n",
    "    Return: \n",
    "    Sig: sigmoid value of array or scalar x.\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# Relu \n",
    "def relu(x): \n",
    "    \"\"\"\n",
    "    Calculates the ReLu of x\n",
    "    \n",
    "    Argument: \n",
    "    x : a scalar of numpy array of any size. \n",
    "    \n",
    "    Return: \n",
    "    relu: returns the relu(x)\n",
    "    \"\"\"\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Druing the calculation of derivative, we need to convert all parameters into on giant vector, which will be implemented by dictionary_to_vector function. We would also need to revert the vector back into its original form, for that we will be using vector_to_dictionary function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to vector.\n",
    "def dictionary_to_vector(parameters): \n",
    "    \"\"\"\n",
    "    Roll all parameters into a single vector.\n",
    "    \n",
    "    Argument: \n",
    "    parameters: a dictionary of parameters \n",
    "    \n",
    "    Return: \n",
    "    theta: a column vector comprising of all parameters stacked.\n",
    "    \"\"\"\n",
    "    index = [0]\n",
    "    initial_index = 0\n",
    "    count =0\n",
    "    for key in parameters.keys(): \n",
    "        new_vector = np.reshape(parameters[key], (-1,1))\n",
    "        end_index = index[-1] + new_vector.shape[0]\n",
    "        index.append(end_index)\n",
    "        if count == 0: \n",
    "            theta = new_vector\n",
    "        else: \n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count +=1\n",
    "    return theta, index\n",
    "\n",
    "# vector to dictionary \n",
    "def vector_to_dictionary(theta, index, param):\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters[\"W1\"] = theta[:index[1]].reshape(param[\"W1\"].shape)\n",
    "    parameters[\"b1\"] = theta[index[1]:index[2]].reshape(param[\"b1\"].shape)\n",
    "    parameters[\"W2\"] = theta[index[2]:index[3]].reshape(param[\"W2\"].shape)\n",
    "    parameters[\"b2\"] = theta[index[3]:index[4]].reshape(param[\"b2\"].shape)\n",
    "    parameters[\"W3\"] = theta[index[4]:index[5]].reshape(param[\"W3\"].shape)\n",
    "    parameters[\"b3\"] = theta[index[5]:index[6]].reshape(param[\"b3\"].shape)\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# gradients to vector \n",
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all our gradients dictionary into a single vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\"]:\n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(gradients[key], (-1,1))\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-D implementation to understand the steps involved \n",
    "\n",
    "# forward propagation\n",
    "def forward_propagation(X, Y, parameters): \n",
    "    \"\"\"\n",
    "    Implements the forward propagation and computes cost.\n",
    "    \n",
    "    Arguments: \n",
    "    X : traininig set examples \n",
    "    Y : true labels \n",
    "    parameters : dictionary of parameters W and b.\n",
    "    \n",
    "    Return: \n",
    "    cost: total cross entropy cost \n",
    "    cache: cache of parameters,pre-activation, activation and bias for all layers.\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # forward step \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    # cost \n",
    "    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1-A3), (1-Y))\n",
    "    cost = (1./m) * np.sum(logprobs)\n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache\n",
    "\n",
    "# backward propagation\n",
    "def backward_propagation(X, Y, cache):\n",
    "    \"\"\"\n",
    "    impelement backward propagation \n",
    "    \n",
    "    Arguments: \n",
    "    X : training set examples \n",
    "    Y : true labels \n",
    "    cache : output cache from forward propagation \n",
    "    \n",
    "    Returns: \n",
    "    gradients: a dictionary of gradient values.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) \n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-D gradient check\n",
    "\n",
    "# gradient check \n",
    "def gradient_check(parameters, gradients, X, Y, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Implements gradient check \n",
    "    \n",
    "    Arguments: \n",
    "    parameters : dictionary of parameters W, b\n",
    "    gradients : dictionary of gradients dW, db\n",
    "    X : input examples \n",
    "    Y : true labels \n",
    "    Epsilon : nudge value for gradient checking \n",
    "    \n",
    "    Returns: \n",
    "    difference : gradient check output\n",
    "    \"\"\"\n",
    "    theta, theta_index = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = theta.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        # Compute J_plus\n",
    "        # for each parameter values, we increase it by epsilon and then calculate the forward\n",
    "        # propagation, so we will iterate over all values of parameters one by one\n",
    "        thetaplus = np.copy(theta)\n",
    "        thetaplus[i][0] = thetaplus[i][0] + epsilon\n",
    "        J_plus[i], _ = forward_propagation(X, Y, vector_to_dictionary(thetaplus, theta_index, parameters))\n",
    "\n",
    "        # Compute J_minus\n",
    "        thetaminus = np.copy(theta)\n",
    "        thetaminus[i][0] = thetaminus[i][0] - epsilon\n",
    "        J_minus[i][0], _ = forward_propagation(X, Y, vector_to_dictionary(thetaminus, theta_index, parameters))\n",
    "        \n",
    "        # gradappr[i]\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i])/ (2*epsilon)\n",
    "        \n",
    "    # compute difference \n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator/ denominator\n",
    "    \n",
    "    if difference > 2e-7:\n",
    "        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "       \n",
    "    return difference\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.1885552035482147e-07\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation(X, Y, parameters)\n",
    "gradients = backward_propagation(X, Y, cache)\n",
    "difference = gradient_check(parameters, gradients, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
